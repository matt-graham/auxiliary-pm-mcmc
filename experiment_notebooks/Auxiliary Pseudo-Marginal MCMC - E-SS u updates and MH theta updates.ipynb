{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import numpy as np\n",
    "import gpdemo.kernels as krn\n",
    "import gpdemo.utils as utils\n",
    "import gpdemo.latent_posterior_approximations as lpa\n",
    "import gpdemo.estimators as est\n",
    "import auxpm.samplers as smp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct data and experiments directorys from environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.environ['DATA_DIR'], 'uci')\n",
    "exp_dir = os.path.join(os.environ['EXP_DIR'], 'apm_mcmc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify main run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_set = 'pima'\n",
    "method = 'apm(ss+mh)'\n",
    "n_chain = 10\n",
    "chain_offset = 0\n",
    "seeds = np.random.random_integers(10000, size=n_chain)\n",
    "n_imp_sample = 1\n",
    "adapt_run = dict(\n",
    "    low_acc_thr = 0.15,\n",
    "    upp_acc_thr = 0.30,\n",
    "    batch_size = 100,\n",
    "    n_batch = 20   \n",
    ")\n",
    "init_log_sigma_prop_scale = 0.5\n",
    "init_log_tau_prop_scale = 0.5\n",
    "n_sample_main = 10000\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and normalise inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.genfromtxt(os.path.join(data_dir, data_set + '_X.txt'))\n",
    "y = np.genfromtxt(os.path.join(data_dir, data_set + '_y.txt'))\n",
    "X, X_mn, X_sd = utils.normalise_inputs(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify prior parameters (data dependent so do after data load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior = dict(\n",
    "    a_tau = 1.,\n",
    "    b_tau = 1. / X.shape[1]**0.5,\n",
    "    a_sigma = 1.1,\n",
    "    b_sigma = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble run parameters into dictionary for recording with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_params = dict(\n",
    "    data_set = data_set,\n",
    "    n_data = X.shape[0],\n",
    "    n_feature = X.shape[1],\n",
    "    method = method,\n",
    "    n_imp_sample = n_imp_sample,\n",
    "    epsilon = epsilon,\n",
    "    prior = prior,\n",
    "    adapt_run = adapt_run,\n",
    "    init_log_sigma_prop_scale = init_log_sigma_prop_scale,\n",
    "    init_log_tau_prop_scale = init_log_tau_prop_scale,\n",
    "    n_sample_main = n_sample_main\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create necessary run objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prng = np.random.RandomState()\n",
    "kernel_func = lambda K, X, theta: (\n",
    "    krn.isotropic_squared_exponential_kernel(K, X, theta, epsilon)\n",
    ")\n",
    "ml_estimator = est.LogMarginalLikelihoodApproxPosteriorISEstimator(\n",
    "    X, y, kernel_func, lpa.laplace_approximation)\n",
    "def log_f_estimator(u, theta=None, cached_res=None):\n",
    "    log_marg_lik_est, new_cached_res = ml_estimator(u, theta, cached_res)\n",
    "    log_prior = (\n",
    "        utils.log_gamma_log_pdf(theta[0], prior['a_sigma'], prior['b_sigma']) +\n",
    "        utils.log_gamma_log_pdf(theta[1], prior['a_tau'], prior['b_tau'])\n",
    "    )\n",
    "    return log_marg_lik_est + log_prior, new_cached_res\n",
    "prop_sampler = lambda theta, prop_scales: np.r_[\n",
    "        theta[0] + prop_scales[0] * prng.normal(), \n",
    "        theta[1] + prop_scales[1] * prng.normal()\n",
    "]\n",
    "log_prop_density = lambda theta_prop, theta_curr, prop_scales: (\n",
    "    -0.5 * (\n",
    "        ((theta_prop[0] - theta_curr[0]) / prop_scales[0])**2 + \n",
    "        ((theta_prop[1] - theta_curr[1]) / prop_scales[1])**2\n",
    "    )\n",
    ")\n",
    "init_prop_scales = np.array([\n",
    "        init_log_sigma_prop_scale, \n",
    "        init_log_tau_prop_scale\n",
    "])\n",
    "sampler = smp.APMEllSSPlusMHSampler(\n",
    "    log_f_estimator, log_prop_density, prop_sampler, init_prop_scales, \n",
    "    lambda: prng.normal(size=(y.shape[0], n_imp_sample)), prng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run chains, starting from random sample from prior in each and saving results to experiments directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(n_chain):\n",
    "    try:\n",
    "        print('Starting chain {0}...'.format(c + 1))\n",
    "        prng.seed(seeds[c])\n",
    "        theta_init = np.array([\n",
    "                np.log(prng.gamma(prior['a_sigma'], 1. / prior['b_sigma'])),\n",
    "                np.log(prng.gamma(prior['a_tau'], 1. / prior['b_tau'])),\n",
    "        ])\n",
    "        sampler.prop_scales = init_prop_scales\n",
    "        print('Starting initial adaptive run...')\n",
    "        adapt_thetas, adapt_prop_scales, adapt_accept_rates = (\n",
    "            sampler.adaptive_run(\n",
    "                theta_init, adapt_run['batch_size'], \n",
    "                adapt_run['n_batch'], adapt_run['low_acc_thr'], \n",
    "                adapt_run['upp_acc_thr'], utils.adapt_factor_func, True\n",
    "            )\n",
    "        )\n",
    "        print('Final proposal scales: {0}'.format(adapt_prop_scales[-1]))\n",
    "        print('Starting main run...')\n",
    "        ml_estimator.reset_cubic_op_count()\n",
    "        start_time = time.clock()\n",
    "        thetas, n_reject = sampler.get_samples(adapt_thetas[-1], n_sample_main)\n",
    "        comp_time = time.clock() - start_time\n",
    "        n_cubic_ops = ml_estimator.n_cubic_ops\n",
    "        tag = '{0}_{1}_chain_{2}'.format(data_set, method, c + 1 + chain_offset)\n",
    "        print('Main run completed: accept rate {0:.1f}%, time {1}s, # cubic ops {2}'\n",
    "             .format((1. - n_reject * 1./ n_sample_main) * 100., comp_time, n_cubic_ops))\n",
    "        utils.save_adaptive_run(exp_dir, tag, adapt_thetas, adapt_prop_scales, \n",
    "                                adapt_accept_rates, thetas, n_reject, \n",
    "                                n_cubic_ops, comp_time, run_params)\n",
    "        utils.plot_trace(thetas)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('Exception encountered')\n",
    "        print(e.message)\n",
    "        print(traceback.format_exc())\n",
    "        print('Skipping to next chain')\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
